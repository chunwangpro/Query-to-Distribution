{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7IKeI51aUJm",
    "outputId": "85df4692-0351-4f17-9268-48219abb7f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/LPALG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m8pTUKVFrrbn"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osXtrhghacXi",
    "outputId": "6d672eb1-4c51-4a57-8f75-9ad959669136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
      "Requirement already satisfied: tensorflow-lattice in /usr/local/lib/python3.7/dist-packages (2.0.10)\n",
      "Requirement already satisfied: pydoe in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.1+cu113)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.21.6)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.0.2)\n",
      "Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (3.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.3.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pydoe) (1.7.3)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->tensorflow-lattice) (0.8.10)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->tensorflow-lattice) (1.14.1)\n",
      "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->tensorflow-lattice) (0.1.7)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->tensorflow-lattice) (4.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow-lattice) (2022.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->tensorflow-lattice) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->tensorflow-lattice) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz graphviz tensorflow-lattice pydoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "M5vOQ7UmSJbm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import common\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "import estimators as estimators_lib\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "def Oracle(table, query):\n",
    "    cols, idxs, ops, vals = query\n",
    "    oracle_est = estimators_lib.Oracle(table)\n",
    "    return oracle_est.Query(cols, ops, vals)\n",
    "\n",
    "\n",
    "def cal_true_card(query, table):\n",
    "    cols, idxs, ops, vals = query\n",
    "    ops = np.array(ops)\n",
    "    probs = Oracle(table, (cols, idxs, ops, vals))\n",
    "    return probs\n",
    "\n",
    "\n",
    "def GenerateQuery(table, min_num_filters, max_num_filters, rng, dataset):\n",
    "    \"\"\"Generate a random query.\"\"\"\n",
    "    num_filters = rng.randint(max_num_filters - 1, max_num_filters)       ######### 这里可以改\n",
    "    cols, idxs, ops, vals = SampleTupleThenRandom(table, num_filters, rng,\n",
    "                                                  dataset)\n",
    "    sel = cal_true_card(\n",
    "        (cols, idxs, ops, vals), table) / float(table.cardinality)\n",
    "    return cols, idxs, ops, vals, sel\n",
    "\n",
    "\n",
    "def SampleTupleThenRandom(table, num_filters, rng, dataset):\n",
    "    vals = []\n",
    "    new_table = table.data\n",
    "    s = new_table.iloc[rng.randint(0, new_table.shape[0])]\n",
    "    vals = s.values\n",
    "\n",
    "    idxs = rng.choice(len(table.columns), replace=False, size=num_filters)   ######## 这里可以改\n",
    "    cols = np.take(table.columns, idxs)\n",
    "    # If dom size >= 10, okay to place a range filter.\n",
    "    # Otherwise, low domain size columns should be queried with equality.\n",
    "    #ops = rng.choice(['='], size=num_filters)\n",
    "    #ops = rng.choice(['<=', '>=', '>', '<'], size=num_filters)\n",
    "    #ops = rng.choice(['<=', '>='], size=num_filters)\n",
    "    ops = rng.choice(['<='], size=num_filters)\n",
    "    #ops_all_eqs = ['='] * num_filters\n",
    "    #sensible_to_do_range = [c.DistributionSize() >= 10 for c in cols]\n",
    "    #ops = np.where(sensible_to_do_range, ops, ops_all_eqs)\n",
    "    # if num_filters == len(table.columns):\n",
    "    #     return table.columns,np.arange(len(table.columns)), ops, vals\n",
    "    vals = vals[idxs]\n",
    "    op_a = []\n",
    "    val_a = []\n",
    "    for i in range(len(vals)):\n",
    "        val_a.append([vals[i]])\n",
    "        op_a.append([ops[i]])\n",
    "    return cols, idxs, pd.DataFrame(op_a).values, pd.DataFrame(val_a).values\n",
    "\n",
    "\n",
    "def dictionary_column_interval(table_size, query_set):\n",
    "    # Traverse all queries to apply the intervalization skill for each column\n",
    "    n_column = table_size[1]\n",
    "    column_interval = {}\n",
    "    for i in range(n_column):\n",
    "        column_interval[i] = set()\n",
    "    for query in query_set:\n",
    "        _, col_idxs, _, vals, _ = query\n",
    "        for i in range(len(col_idxs)):\n",
    "            column_interval[col_idxs[i]].add(vals[i][0])\n",
    "    for k, v in column_interval.items():\n",
    "        column_interval[k] = sorted(list(v))\n",
    "        least, great = column_interval[k][0], column_interval[k][-1]\n",
    "        column_interval[k] = sorted([0, least/2] + column_interval[k] + [great+1])\n",
    "    return column_interval\n",
    "\n",
    "\n",
    "def count_column_unique_interval(unique_intervals):\n",
    "    # count unique query interval in each column\n",
    "    return [len(v) for v in unique_intervals.values()]\n",
    "\n",
    "\n",
    "def process_train_data(unique_intervals, query_set, train_size=1):\n",
    "    train_size = 1\n",
    "    X, Y = [], []\n",
    "    origin = np.array([[0, v[-1]] for v in unique_intervals.values()]).ravel()\n",
    "    for query in query_set:\n",
    "        x = list(origin)\n",
    "        _, col_idxs, ops, vals, sel = query\n",
    "        for i in range(len(col_idxs)):\n",
    "            if ops[i][0] == \"<=\":\n",
    "                x[col_idxs[i]*2+1] = vals[i][0]\n",
    "            elif ops[i][0] == \"<\":\n",
    "                ind = unique_intervals[col_idxs[i]].index(vals[i][0]) - 1\n",
    "                x[col_idxs[i]*2+1] = unique_intervals[col_idxs[i]][ind]\n",
    "            elif ops[i][0] == \">\":\n",
    "                x[col_idxs[i]*2] = vals[i][0]\n",
    "            elif ops[i][0] == \">=\":\n",
    "                ind = unique_intervals[col_idxs[i]].index(vals[i][0]) + 1\n",
    "                x[col_idxs[i]*2] = unique_intervals[col_idxs[i]][ind]\n",
    "            elif ops[i][0] == \"=\":\n",
    "                ind = unique_intervals[col_idxs[i]].index(vals[i][0]) - 1\n",
    "                x[col_idxs[i]*2] = unique_intervals[col_idxs[i]][ind]\n",
    "                x[col_idxs[i]*2+1] = vals[i][0]\n",
    "        X.append(x)\n",
    "        Y.append(sel)\n",
    "    X = np.array(X).astype(np.float32)\n",
    "    Y = np.array(Y).astype(np.float32).reshape(-1, 1)\n",
    "    total = np.concatenate((X, Y), axis=1)\n",
    "    # total = np.unique(total, axis=0)\n",
    "#     choose = np.random.choice(total.shape[0], size=round(total.shape[0]*train_size), replace=False)\n",
    "#     others = list(set(range(total.shape[0])) - set(choose))\n",
    "#     train, test = total[choose], total[others]\n",
    "#     df_train = pd.DataFrame(train, columns=[f'col_{i}' for i in range(total.shape[1])])\n",
    "    df_train = pd.DataFrame(total, columns=[f'col_{i}' for i in range(total.shape[1])])\n",
    "    # boundary\n",
    "    # df_train.loc[len(df_train.index)] = [0] * total.shape[1]\n",
    "    # zero = [[v[-1], 0] for v in unique_intervals.values()]\n",
    "    # df_train.loc[len(df_train.index)] = list(np.array(zero).ravel()) + [0.0]\n",
    "    # one = [[0, v[-1]] for v in unique_intervals.values()]\n",
    "    # df_train.loc[len(df_train.index)] = list(np.array(one).ravel()) + [1.0]\n",
    "    \n",
    "    new_train = np.array(df_train.sort_values(by=list(df_train.columns)[:-1]))\n",
    "    train_X, train_Y = np.hsplit(new_train, [-1])\n",
    "\n",
    "#     df_test = pd.DataFrame(test, columns=[f'col_{i}' for i in range(total.shape[1])])\n",
    "#     new_test = np.array(df_test.sort_values(by=list(df_test.columns)[:-1]))\n",
    "#     test_X, test_Y = np.hsplit(new_test, [-1])\n",
    "    return train_X, train_Y  # , test_X, test_Y\n",
    "\n",
    "\n",
    "def generate_data_new(grid, trainer):\n",
    "    assert grid.shape[1] == n_column\n",
    "    # transform 1-input grid to 2-input extend grid\n",
    "    length = grid.shape[0]\n",
    "    inf = [0] * length\n",
    "    grid_dict = {}\n",
    "    for i in range(n_column):\n",
    "        grid_dict[f'col_{i}_inf'] = inf\n",
    "        grid_dict[f'col_{i}_sup'] = grid[:, i]\n",
    "    extend_grid = np.array(pd.DataFrame(grid_dict))\n",
    "    print(\"Begin model inference\")\n",
    "    pred = trainer.inference(extend_grid)\n",
    "    print(\"Done\")\n",
    "    # newpred is the predict cardinality\n",
    "    pred = pred[0]\n",
    "    newpred = np.round(pred*n_row)\n",
    "    #newpred = np.round(pred)\n",
    "    # delete all the zero cardinality rows\n",
    "    line = pd.DataFrame(\n",
    "        np.concatenate([grid, newpred], axis=1),\n",
    "        columns=[f'col_{i}' for i in range(n_column)]+['card']\n",
    "    )\n",
    "    nozero = (line == 0).sum(axis=1)\n",
    "    line = line[nozero==0].reset_index(drop=True)\n",
    "    grid, pred = np.hsplit(np.array(line), [-1])\n",
    "    pred = pred.astype(np.int32)\n",
    "    # generate dataNew\n",
    "    print(\"\\nBegin generating table...\")\n",
    "    dataNew = pd.DataFrame(\n",
    "        columns=[f'col_{i}' for i in range(n_column)],\n",
    "        index=range(n_row)\n",
    "    )\n",
    "    count = 0\n",
    "    for i in trange(grid.shape[0]):\n",
    "        df = dataNew\n",
    "        grid_value = grid[i]\n",
    "        for j in range(n_column):\n",
    "            df = df.query(f'col_{j} <= {grid_value[j]}')\n",
    "        card = pred[i][0] - df.shape[0]\n",
    "        if card > 0:\n",
    "            #df3 = pd.DataFrame({f\"col_{k}\": [grid_value[k]] * card for k in range(n_column)})\n",
    "            #dataNew = dataNew.append(df3, ignore_index = True)\n",
    "            dataNew.iloc[count:count + card, :] = grid_value\n",
    "            count += card\n",
    "            if count > n_row:\n",
    "                print(\"Reached table length in \", i, grid.shape[0])\n",
    "                break\n",
    "        # print table length every 5000\n",
    "        if i % 5000 == 0:\n",
    "            print(count)\n",
    "    dataNew.dropna(axis=0, how='all', inplace=True)\n",
    "    return dataNew\n",
    "    \n",
    "    \n",
    "def execute_query(dataNew, query_set):\n",
    "    diff = []\n",
    "    for query in tqdm(query_set):\n",
    "        df = dataNew\n",
    "        _, col_idxs, ops, vals, sel = query\n",
    "        for i in range(len(col_idxs)):\n",
    "            op = '==' if ops[i][0] == \"=\" else ops[i][0]\n",
    "            df = df.query(f'col_{col_idxs[i]} {op} {vals[i][0]}')\n",
    "        card = 1 if round(sel * n_row) == 0 else round(sel * n_row)\n",
    "        card2 = 1 if df.shape[0] == 0 else df.shape[0]\n",
    "        diff.append(max(card/card2, card2/card))\n",
    "    return diff\n",
    "\n",
    "\n",
    "def print_error(diff, args):\n",
    "    print(\n",
    "        f\"\\n\\n Q-error of Lattice (query size={args.query_size}, condition={args.num_conditions}, epoches={args.epochs}):\\n\"\n",
    "    )\n",
    "    print(f\"min:    {np.min(diff)}\")\n",
    "    print(f\"10:     {np.percentile(diff, 10)}\")\n",
    "    print(f\"20:     {np.percentile(diff, 20)}\")\n",
    "    print(f\"30:     {np.percentile(diff, 30)}\")\n",
    "    print(f\"40:     {np.percentile(diff, 40)}\")\n",
    "    print(f\"median: {np.median(diff)}\")\n",
    "    print(f\"60:     {np.percentile(diff, 60)}\")\n",
    "    print(f\"70:     {np.percentile(diff, 70)}\")\n",
    "    print(f\"80:     {np.percentile(diff, 80)}\")\n",
    "    print(f\"90:     {np.percentile(diff, 90)}\")\n",
    "    print(f\"95:     {np.percentile(diff, 95)}\")\n",
    "    print(f\"max:    {np.max(diff)}\")\n",
    "    print(f\"mean:   {np.mean(diff)}\")\n",
    "    \n",
    "    \n",
    "def calc_time(tic, toc):\n",
    "    total_time = toc - tic\n",
    "    m, s = divmod(total_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:0>2.0f}:{m:0>2.0f}:{s:0>2.0f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "t0Uw0Ge-ycLu"
   },
   "outputs": [],
   "source": [
    "class ResBlock(tf.keras.Model):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out, act_name='relu'):\n",
    "        super().__init__()\n",
    "        assert dim_in == dim_out\n",
    "        block = tf.keras.models.Sequential()\n",
    "        block.add(tf.keras.layers.Dense(dim_in))\n",
    "        block.add(tf.keras.layers.BatchNormalization())\n",
    "        block.add(tf.keras.layers.Activation(act_name))\n",
    "        block.add(tf.keras.layers.Dense(dim_hidden))\n",
    "        block.add(tf.keras.layers.BatchNormalization())\n",
    "        block.add(tf.keras.layers.Activation(act_name))\n",
    "        block.add(tf.keras.layers.Dense(dim_out))\n",
    "        block.add(tf.keras.layers.BatchNormalization())\n",
    "        self.block = block\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        return tf.nn.relu(identity + out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "sZvNz2H2Tr5I"
   },
   "outputs": [],
   "source": [
    "class CopulaLayer(tf.keras.Model):\n",
    "    def __init__(self, dim, res_dim=[2, 10, 1], res_blocks=4, act_name='relu', lattice_size=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.lattice_size = lattice_size\n",
    "        \n",
    "        self.copula_lattice = tfl.layers.Lattice(\n",
    "            lattice_sizes=[self.lattice_size] * self.dim,\n",
    "            interpolation='hypercube',  # simplex\n",
    "            monotonicities=['increasing'] * self.dim,\n",
    "            output_min= 0.0,\n",
    "            output_max= 1.0,\n",
    "            name='lattice',\n",
    "        )\n",
    "\n",
    "        dim_in, dim_hidden, dim_out = res_dim\n",
    "        assert dim_out == 1\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(dim_hidden))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "        for i in range(res_blocks):\n",
    "            model.add(ResBlock(dim_hidden, dim_hidden, dim_hidden, act_name=act_name))\n",
    "        model.add(tf.keras.layers.Dense(dim_out))\n",
    "        self.Res_model = model\n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim,\n",
    "            \"lattice_size\": self.lattice_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # return self.copula_lattice(x)\n",
    "        #x = tf.keras.layers.concatenate(x)\n",
    "        return self.Res_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "V-zEzNria55G"
   },
   "outputs": [],
   "source": [
    "class LatticeCopulaModel(tf.keras.Model):\n",
    "    def __init__(self, modelpath, dim, res_dim=[2, 10, 1], res_blocks=4, act_name='relu', lattice_size=2, pwl_keypoints=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.model_path = modelpath\n",
    "        self.pwl_calibration_input_keypoints = pwl_keypoints\n",
    "        self.copula = CopulaLayer(dim=dim, res_dim=[2, 10, 1], res_blocks=4, act_name='relu', lattice_size=lattice_size)\n",
    "        \n",
    "        self.cdf_inputs = []\n",
    "        for i in range(self.dim):\n",
    "            self.cdf_inputs.append(\n",
    "                tf.keras.layers.Input(shape=[1], name='col_%s_inf' % i))\n",
    "            self.cdf_inputs.append(\n",
    "                tf.keras.layers.Input(shape=[1], name='col_%s_sup' % i))\n",
    "\n",
    "        self.calibrators = []\n",
    "        for i in range(self.dim):\n",
    "            self.calibrators.append(\n",
    "                tfl.layers.PWLCalibration(\n",
    "                    input_keypoints=np.array(\n",
    "                        self.pwl_calibration_input_keypoints[i]),\n",
    "                    dtype=tf.float32,\n",
    "                    output_min=0.0,\n",
    "                    output_max=1.0,\n",
    "                    #clamp_min=True,\n",
    "                    #clamp_max=True,\n",
    "                    monotonicity='decreasing',\n",
    "                    name='col_%s_inf_pwl' % i\n",
    "                )(self.cdf_inputs[2*i]))\n",
    "            self.calibrators.append(\n",
    "                tfl.layers.PWLCalibration(\n",
    "                    input_keypoints=np.array(\n",
    "                        self.pwl_calibration_input_keypoints[i]),\n",
    "                    # input_keypoints=np.linspace(\n",
    "                    #     feat_mins[i],\n",
    "                    #     feat_maxs[i],\n",
    "                    #     num=pwl_calibration_num_keypoints),\n",
    "                    dtype=tf.float32,\n",
    "                    output_min=0.0,\n",
    "                    output_max=1.0,\n",
    "                    #clamp_min=True,\n",
    "                    #clamp_max=True,\n",
    "                    monotonicity='increasing',\n",
    "                    name='col_%s_sup_pwl' % i\n",
    "                )(self.cdf_inputs[2*i+1]))\n",
    "\n",
    "        self.lattice_cdf = []\n",
    "        for i in range(self.dim):\n",
    "            self.lattice_cdf.append(\n",
    "                tfl.layers.Lattice(\n",
    "                    lattice_sizes=[lattice_size] * 2,\n",
    "                    interpolation='hypercube',  # simplex\n",
    "                    monotonicities=['increasing'] * 2,\n",
    "                    output_min= 0.0,\n",
    "                    output_max= 1.0,\n",
    "                    name='lattice_col_%s' % i,\n",
    "                )([self.calibrators[2*i], self.calibrators[2*i+1]]))\n",
    "            \n",
    "        self.cdf_model = tf.keras.models.Model(\n",
    "            inputs=self.cdf_inputs,\n",
    "            outputs= self.lattice_cdf\n",
    "        )\n",
    "\n",
    "\n",
    "        self.model = tf.keras.models.Model(\n",
    "            inputs=self.cdf_inputs,\n",
    "            outputs= self.copula(tf.keras.layers.concatenate(self.lattice_cdf))\n",
    "        )\n",
    "        # self.model.save('%s.hdf5' % self.model_path)\n",
    "        self.model.summary()\n",
    "    \n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.cdf_model(inputs)\n",
    "        x_reshape = tf.keras.layers.concatenate(x)\n",
    "        y = self.copula(x_reshape)\n",
    "        grad = y\n",
    "        for i in range(self.dim):\n",
    "            grad = tf.gradients(grad, x[i])\n",
    "            grad = grad[0]\n",
    "        return y, x, grad\n",
    "        \n",
    "\n",
    "    def compile(self, loss, optimizer):\n",
    "        super().compile()\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "\n",
    "    def train_step(self, data, training=True):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, lattice_inputs, lattice_grad = self(x)\n",
    "            loss1 = tf.reshape(self.loss(y, y_pred), [-1, 1])\n",
    "            loss2 = tf.nn.relu(y_pred - tf.keras.backend.min(lattice_inputs))\n",
    "            lb = tf.math.maximum(sum(lattice_inputs) - self.dim + 1, 0)\n",
    "            loss3 = tf.nn.relu(lb - y_pred)\n",
    "            loss4 = tf.nn.relu(-lattice_grad)\n",
    "            loss = tf.reduce_sum(loss1 + loss2 + loss3 + loss4)\n",
    "        trainable_vars = self.trainable_weights\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        return {\"loss\": loss, \"loss1\": loss1, \"loss2\": loss2, \"loss3\": loss3, \"loss4\": loss4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "yd2DHdvka-AF"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, name, dim, pwl_keypoints=None, lattice_size=2):\n",
    "        self.dim = dim\n",
    "        self.model_path = './models/Lattice/model/' + name\n",
    "        self.weight_path = './models/Lattice/weight/' + name\n",
    "        self.pwl_keypoints = unique_intervals if pwl_keypoints is None else pwl_keypoints\n",
    "        self.model = LatticeCopulaModel(modelpath=self.model_path, dim=dim, lattice_size=2, pwl_keypoints=self.pwl_keypoints)\n",
    "        # self.model.build(input_shape=[(None, 1) for i in range(2*self.dim)])\n",
    "        # self.model.summary()\n",
    "        \n",
    "    def train(self,\n",
    "              X,\n",
    "              y,\n",
    "              lr=0.01,\n",
    "              bs=20,\n",
    "              epochs=3000,\n",
    "              reduceLR_factor=0.5,\n",
    "              reduceLR_patience=20,\n",
    "              earlyStopping_patience=100,\n",
    "              verbose=1,\n",
    "              loss='MSE',\n",
    "              opt='Adam'):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "        features = [X[:, i] for i in range(X.shape[1])]\n",
    "        target = y\n",
    "        Loss = {\n",
    "            'MAE': tf.keras.losses.mean_absolute_error,\n",
    "            'MSE': tf.keras.losses.mean_squared_error,\n",
    "            'MAPE': tf.keras.losses.mean_absolute_percentage_error\n",
    "        }\n",
    "\n",
    "        Opt = {\n",
    "            'Adam': tf.keras.optimizers.Adam(),\n",
    "            'Nadam': tf.keras.optimizers.Nadam(),\n",
    "            'Adagrad': tf.keras.optimizers.Adagrad(),\n",
    "            'Adadelta': tf.keras.optimizers.Adadelta(),\n",
    "            'Adamax': tf.keras.optimizers.Adamax(),\n",
    "            'RMSprop': tf.keras.optimizers.RMSprop(),\n",
    "        }\n",
    "        self.model.compile(loss=Loss[loss], optimizer=Opt[opt])\n",
    "\n",
    "        #self.model.save('%s' % self.model_path, save_format='tf')\n",
    "        \n",
    "        \n",
    "\n",
    "        earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                         patience=earlyStopping_patience,\n",
    "                                                         verbose=verbose,\n",
    "                                                         mode='min')\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint('%s.hdf5' %\n",
    "                                                      self.weight_path,\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor='loss',\n",
    "                                                      mode='min',\n",
    "                                                      save_weights_only=True)\n",
    "        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=reduceLR_factor,\n",
    "            patience=reduceLR_patience,\n",
    "            verbose=verbose,\n",
    "            min_delta=1e-15,\n",
    "            mode='min')\n",
    "\n",
    "        self.model.fit(features,\n",
    "                       target,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=bs,\n",
    "                       verbose=1,\n",
    "                       callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n",
    "                       )\n",
    "        # self.model.build(input_shape = (None, self.dim*2))\n",
    "        # self.model.load_weights('%s.hdf5' % self.weight_path, custom_objects={\"Model\": Model})\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        self.model = tf.keras.models.load_model('%s.hdf5' % self.model_path)\n",
    "        self.model.load_weights('%s.hdf5' % self.weight_path)\n",
    "\n",
    "        \n",
    "    def inference(self, grid):\n",
    "        assert grid.shape[1] == self.dim * 2\n",
    "        pred = self.model.predict(np.hsplit(grid, self.dim * 2))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b8a6da82a7dd438eb72ba8e74cc90745",
      "211d064cac7a430da52db2cfb448ea06",
      "896440d59a134bb0a8ace709ab4410a5",
      "f45a5f6d9cd84295b599e03cb4dcd6d0",
      "d2963fc5eef546c9909c8a1522cd846b",
      "121ab2883680414bb920cf5a7a58c3d8",
      "e0cb375a0fe34714b94475b4e7be5cf8",
      "2acd7059b09540299004d13aa1335938",
      "7bc2a5e57b8e4d75b938e1a7d4d6090a",
      "3120814aad8045528e6ebc1bbe12cebc",
      "6f3655be420c4acda3b52bb5c59470f6"
     ]
    },
    "id": "mW2SpqH7bFen",
    "outputId": "5b7dc47f-2195-4afb-8f7a-d84c253b894d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset wine2.csv done\n",
      "(6497, 2)\n",
      "(6497, 2)\n",
      "0\n",
      "1\n",
      "0 106\n",
      "1 187\n",
      "Begin Generating Queries ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a6da82a7dd438eb72ba8e74cc90745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Generating Queries.\n",
      "\n",
      "\n",
      "Calculating intervalization...\n",
      "\n",
      "Column intervals [85, 112] 9520\n",
      "\n",
      "\n",
      "Building Lattice...\n",
      "Model: \"model_95\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " col_0_inf (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " col_0_sup (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " col_1_inf (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " col_1_sup (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " col_0_inf_pwl (PWLCalibration)  (None, 1)           85          ['col_0_inf[0][0]']              \n",
      "                                                                                                  \n",
      " col_0_sup_pwl (PWLCalibration)  (None, 1)           85          ['col_0_sup[0][0]']              \n",
      "                                                                                                  \n",
      " col_1_inf_pwl (PWLCalibration)  (None, 1)           112         ['col_1_inf[0][0]']              \n",
      "                                                                                                  \n",
      " col_1_sup_pwl (PWLCalibration)  (None, 1)           112         ['col_1_sup[0][0]']              \n",
      "                                                                                                  \n",
      " lattice_col_0 (Lattice)        (None, 1)            4           ['col_0_inf_pwl[0][0]',          \n",
      "                                                                  'col_0_sup_pwl[0][0]']          \n",
      "                                                                                                  \n",
      " lattice_col_1 (Lattice)        (None, 1)            4           ['col_1_inf_pwl[0][0]',          \n",
      "                                                                  'col_1_sup_pwl[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 2)            0           ['lattice_col_0[0][0]',          \n",
      "                                                                  'lattice_col_1[0][0]']          \n",
      "                                                                                                  \n",
      " copula_layer_50 (CopulaLayer)  (None, 1)            1881        ['concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,283\n",
      "Trainable params: 2,023\n",
      "Non-trainable params: 260\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "Lattice is already built, begin training...\n",
      "\n",
      "Epoch 1/5000\n",
      "107\n",
      "109\n",
      "Tensor(\"lattice_copula_model_51/copula_layer_50/sequential_172/dense_473/BiasAdd:0\", shape=(1000, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset',\n",
    "                        type=str,\n",
    "                        default='wine2',\n",
    "                        help='Dataset.')\n",
    "    parser.add_argument('--loss', type=str, default='MSE', help='Loss.')\n",
    "    parser.add_argument('--opt', type=str, default='Adam', help='Optimizer.')\n",
    "    parser.add_argument('--query-size',\n",
    "                        type=int,\n",
    "                        default=2000,\n",
    "                        help='query size')\n",
    "    parser.add_argument('--num-conditions',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='num of conditions')\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=5000,\n",
    "                        help='Number of epochs to train for.')\n",
    "    parser.add_argument('--lhs_n',\n",
    "                        type=int,\n",
    "                        default=10000,\n",
    "                        help='Number of lhs samples to generate for.')\n",
    "    parser.add_argument('--train-size',\n",
    "                        type=float,\n",
    "                        default=0.8,\n",
    "                        help='train size')\n",
    "    parser.add_argument('--bs', type=int, default=1000, help='Batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate')\n",
    "    parser.add_argument('--lattice', type=int, default=2, help='Lattice size.')\n",
    "    parser.add_argument('--seed', type=int, default=4321, help='Random seed')\n",
    "    parser.add_argument('--sample',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help='reload trained mode')\n",
    "    #args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    bs = int(args.bs)\n",
    "    lr = float(args.lr)\n",
    "    train_size = float(args.train_size)\n",
    "    epochs = int(args.epochs)\n",
    "    lattice = int(args.lattice)\n",
    "    sample = int(args.sample)\n",
    "    lhs_n = int(args.lhs_n)\n",
    "\n",
    "    type_casts = {}\n",
    "    table = datasets.LoadDataset(args.dataset + '.csv',\n",
    "                                 args.dataset,\n",
    "                                 type_casts=type_casts)\n",
    "    \n",
    "    print('Begin Generating Queries ...')\n",
    "    time0 = time.time()\n",
    "    rng = np.random.RandomState(args.seed)\n",
    "    query_set = [\n",
    "        GenerateQuery(table, 2, args.num_conditions + 1, rng, args.dataset)\n",
    "        for i in trange(args.query_size)\n",
    "    ]\n",
    "    print('Complete Generating Queries.')\n",
    "\n",
    "    print(\"\\n\\nCalculating intervalization...\")\n",
    "    time1 = time.time()\n",
    "    table_size = table.data.shape\n",
    "    n_row, n_column = table_size[0], table_size[1]\n",
    "    unique_intervals = dictionary_column_interval(table_size, query_set)\n",
    "    column_interval_number = count_column_unique_interval(unique_intervals)\n",
    "    print(\"\\nColumn intervals\", column_interval_number, np.product(column_interval_number))\n",
    "    \n",
    "    print(\"\\n\\nBuilding Lattice...\")\n",
    "    train_X, train_Y = process_train_data(unique_intervals, query_set)\n",
    "    # train_X, train_Y, test_X, test_Y = process_train_data(unique_intervals, query_set)\n",
    "    # print(\"  Total query:\", args.query_size)\n",
    "    # print(\"  Train query:\", train_X.shape[0])\n",
    "    # print(\"  Test  query:\", test_X.shape[0])\n",
    "    # print(\"\\n\\n\")\n",
    "    name = f\"{args.dataset}_{args.query_size}query_{args.num_conditions}column_{args.epochs}epoch\"\n",
    "    #model = LatticeCDF(unique_intervals, pwl_keypoints=None)\n",
    "    m = Trainer(name, n_column, pwl_keypoints=None)\n",
    "    \n",
    "    print(\"\\n\\nLattice is already built, begin training...\\n\")\n",
    "    time2 = time.time()\n",
    "    #bs = train_X.shape[0]\n",
    "    m.train(train_X,\n",
    "            train_Y,\n",
    "            lr=lr,\n",
    "            bs=bs,\n",
    "            epochs=epochs,\n",
    "            loss=args.loss,\n",
    "            opt=args.opt\n",
    "            )\n",
    "    \n",
    "    print(\"\\nFinish training\")\n",
    "    time3 = time.time()\n",
    "    # Full-Factorial net of unique intervals\n",
    "#     values = [v for v in unique_intervals.values()]\n",
    "#     mesh = np.meshgrid(*values)\n",
    "#     grid = np.array(mesh).T.reshape(-1, len(values)).astype(np.float32)\n",
    "    \n",
    "    # Latin Hypercube sampling\n",
    "#     lb = np.array([v[1] for v in unique_intervals.values()])\n",
    "#     ub = np.array([v[-1] for v in unique_intervals.values()])\n",
    "#     lhs_sample = lhs(n_column, samples=10000, criterion='center')\n",
    "#     sample_df = pd.DataFrame(lb + (ub-lb)*lhs_sample, columns=[f'col_{i}' for i in range(n_column)])\n",
    "#     grid = np.array(sample_df.sort_values(by=list(sample_df.columns)))\n",
    "    \n",
    "    \n",
    "    lb = np.array([1] * n_column)\n",
    "    ub = np.array(column_interval_number) - 1\n",
    "    lhs_sample = lb + (ub - lb) * lhs(n_column, samples=lhs_n, criterion='center')\n",
    "    index = np.round(lhs_sample).astype(np.int32)\n",
    "    grid_mesh = np.empty_like(index, dtype=np.float32)\n",
    "    for i in range(lhs_n):\n",
    "        idx = index[i, :]\n",
    "        grid_mesh[i] = [unique_intervals[j][idx[j]] for j in range(n_column)]\n",
    "    sample_df = pd.DataFrame(grid_mesh, columns=[f'col_{i}' for i in range(n_column)])\n",
    "    grid_a = np.array(sample_df.sort_values(by=list(sample_df.columns)))\n",
    "    greatest = np.array([v[-1] for v in unique_intervals.values()]).reshape(1, -1)\n",
    "    grid = np.concatenate([grid_a, greatest], axis=0)\n",
    "    dataNew = generate_data_new(grid, trainer=m)\n",
    "\n",
    "    print(\"\\nFinish generate table, calculating Q-error on new table...\")\n",
    "    time4 = time.time()\n",
    "    diff = execute_query(dataNew, query_set)\n",
    "    print_error(diff, args)\n",
    "\n",
    "    print(f\"\\noriginal table shape: {table_size}\")\n",
    "    print(f\"  Our table shape   : {dataNew.shape}\")\n",
    "    time5 = time.time()\n",
    "\n",
    "    print(\"\\nTime passed:\")\n",
    "    print(\" Generate Query  :  \", calc_time(time0, time1))\n",
    "    print(\" Build  Lattice  :  \", calc_time(time1, time2))\n",
    "    print(\"   Training      :  \", calc_time(time2, time3))\n",
    "    print(\"Generate New Data:  \", calc_time(time3, time4))\n",
    "    print(\"Calculate Q-error:  \", calc_time(time4, time5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtxjzk6aPNcX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "121ab2883680414bb920cf5a7a58c3d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "211d064cac7a430da52db2cfb448ea06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_121ab2883680414bb920cf5a7a58c3d8",
      "placeholder": "​",
      "style": "IPY_MODEL_e0cb375a0fe34714b94475b4e7be5cf8",
      "value": "100%"
     }
    },
    "2acd7059b09540299004d13aa1335938": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3120814aad8045528e6ebc1bbe12cebc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f3655be420c4acda3b52bb5c59470f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bc2a5e57b8e4d75b938e1a7d4d6090a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "896440d59a134bb0a8ace709ab4410a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2acd7059b09540299004d13aa1335938",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bc2a5e57b8e4d75b938e1a7d4d6090a",
      "value": 2000
     }
    },
    "b8a6da82a7dd438eb72ba8e74cc90745": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_211d064cac7a430da52db2cfb448ea06",
       "IPY_MODEL_896440d59a134bb0a8ace709ab4410a5",
       "IPY_MODEL_f45a5f6d9cd84295b599e03cb4dcd6d0"
      ],
      "layout": "IPY_MODEL_d2963fc5eef546c9909c8a1522cd846b"
     }
    },
    "d2963fc5eef546c9909c8a1522cd846b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0cb375a0fe34714b94475b4e7be5cf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f45a5f6d9cd84295b599e03cb4dcd6d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3120814aad8045528e6ebc1bbe12cebc",
      "placeholder": "​",
      "style": "IPY_MODEL_6f3655be420c4acda3b52bb5c59470f6",
      "value": " 2000/2000 [00:03&lt;00:00, 595.15it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
