{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5516e8a0",
   "metadata": {},
   "source": [
    "# PWL - Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95cbdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/rlw36ttd3d9dvskjy8fd9t3r0000gn/T/ipykernel_42458/1248846584.py:9: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# import common\n",
    "import argparse\n",
    "\n",
    "# import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import estimators as estimators_lib\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "\n",
    "from query_func import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"test-2\", help=\"Dataset.\")\n",
    "parser.add_argument(\"--query-size\", type=int, default=10000, help=\"query size\")\n",
    "parser.add_argument(\"--min-conditions\", type=int, default=1, help=\"min num of conditions\")\n",
    "parser.add_argument(\"--max-conditions\", type=int, default=2, help=\"max num of conditions\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs to train for.\")\n",
    "parser.add_argument(\"--bs\", type=int, default=1000, help=\"Batch size.\")\n",
    "parser.add_argument(\"--loss\", type=str, default=\"MSE\", help=\"Loss.\")\n",
    "parser.add_argument(\"--lattice-size\", type=int, default=2, help=\"Lattice size.\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "except:\n",
    "    # args = parser.parse_args([])\n",
    "    args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "OPS = {\">\": np.greater, \"<\": np.less, \">=\": np.greater_equal, \"<=\": np.less_equal, \"=\": np.equal}\n",
    "FilePath = (\n",
    "    f\"{args.dataset}_{args.query_size}_({args.min_conditions}, {args.max_conditions})_{args.loss}\"\n",
    ")\n",
    "resultsPath = f\"results/{FilePath}\"\n",
    "modelPath = f\"saved_models/{FilePath}\"\n",
    "make_directory(resultsPath)\n",
    "make_directory(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77627d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_row, n_column 要被table_size 取代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f44992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin Loading Data ...\")\n",
    "print(f\"{args.dataset}.csv\")\n",
    "table = np.loadtxt(f\"datasets/{args.dataset}.csv\", delimiter=\",\")\n",
    "np.savetxt(f\"{resultsPath}/original_table.csv\", table, delimiter=\",\")\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "print(\"Begin Generating Queries Set...\")\n",
    "table_size = table.shape\n",
    "n_row, n_column = table_size\n",
    "rng = np.random.RandomState(args.seed)\n",
    "query_set = [\n",
    "    generate_random_query(table, args.min_conditions, args.max_conditions + 1, rng)\n",
    "    for _ in tqdm(range(args.query_size))\n",
    "]\n",
    "print(\"Done.\\n\")\n",
    "print(\"Begin Intervalization ...\")\n",
    "unique_intervals = column_intervalization(table_size, query_set)\n",
    "unique_intervals\n",
    "column_interval_number = count_column_unique_interval(unique_intervals)\n",
    "print(f\"{column_interval_number=}\")\n",
    "print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5790a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = datasets.LoadDataset(f\"datasets/{args.dataset}.csv\", args.dataset)\n",
    "\n",
    "# print(\"Begin Generating Queries Set...\")\n",
    "# rng = np.random.RandomState(args.seed)\n",
    "# query_set = [\n",
    "#     GenerateQuery(table, args.min_conditions, args.max_conditions + 1, rng, args.dataset)\n",
    "#     for _ in tqdm(range(args.query_size))\n",
    "# ]\n",
    "# print(\"Done.\\n\")\n",
    "\n",
    "\n",
    "# table_size = table.data.shape\n",
    "# n_row = table_size[0]\n",
    "# n_column = table_size[1]\n",
    "# print(\"Begin Intervalization ...\")\n",
    "# unique_intervals = dictionary_column_interval(table_size, query_set)\n",
    "# column_interval_number = count_column_unique_interval(unique_intervals)\n",
    "# print(\"Done.\\n\")\n",
    "# print(column_interval_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改 x = [sys.maxsize] * n_column     # 这里使用每个col_unique_interval的最后一个元素即可\n",
    "# 如果使用两个input的话，一个修改为最大，一个修改为最小\n",
    "train_X = []\n",
    "for query in query_set:\n",
    "    x = [sys.maxsize] * n_column  # 这里使用每个col_unique_interval的最后一个元素即可\n",
    "    idxs, _, vals, _ = query\n",
    "    for i in range(len(idxs)):\n",
    "        x[idxs[i]] = vals[i]\n",
    "    train_X.append(x)\n",
    "train_X = np.array(train_X).astype(np.float32)\n",
    "train_Y = np.array([[query[-1]] for query in query_set], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aac810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = []\n",
    "# train_Y = []\n",
    "# for query in query_set:\n",
    "#     x = [sys.maxsize] * n_column  # 这里使用每个col_unique_interval的最后一个元素即可\n",
    "#     _, idxs, _, vals, sel = query\n",
    "#     for i in range(len(idxs)):\n",
    "#         x[idxs[i]] = vals[i][0]\n",
    "#     train_X.append(x)\n",
    "#     train_Y.append(sel)\n",
    "\n",
    "# train_X = np.array(train_X).astype(np.float32)\n",
    "# train_Y = np.array(train_Y).astype(np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c82915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train set unique\n",
    "# train = np.concatenate((train_X, train_Y), axis=1)\n",
    "# train = np.unique(train, axis=0)\n",
    "# train_X, train_Y = np.hsplit(train, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以PWL改成三次样条吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f24ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PWLLattice(\n",
    "    modelPath,\n",
    "    table_size,\n",
    "    unique_intervals,\n",
    "    pwl_keypoints=None,\n",
    "    lattice_size=args.lattice_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89220d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(train_X, train_Y, lr=args.lr, bs=args.bs, epochs=args.epochs, loss=args.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda671da",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [v for v in unique_intervals.values()]\n",
    "mesh = np.meshgrid(*values)  # 所有 unique interval 的笛卡尔积网格\n",
    "grid = np.array(mesh).T.reshape(-1, len(values)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load()\n",
    "grid_pred = m.predict(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = m.generate(grid, grid_pred)\n",
    "np.savetxt(f\"{resultsPath}/generated_table.csv\", dataNew, delimiter=\",\")\n",
    "Q_error = calculate_Q_error(dataNew, query_set, table_size)\n",
    "print_Q_error(Q_error, args, resultsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(dataNew[:, 0], dataNew[:, 1], \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4730e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f83a421",
   "metadata": {},
   "source": [
    "### 测试简单分布中，使用generate是否能得到正确的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[2, 9], [5, 4], [3, 1], [9, 3], [2, 9], [2, 9], [3, 10], [9, 1], [10, 1], [10, 1]])\n",
    "values = [range(1, 11), range(1, 11)]\n",
    "mesh = np.meshgrid(*values)  # 所有unique interval 的笛卡尔积网格\n",
    "grid = np.array(mesh).T.reshape(-1, len(values)).astype(np.float32)\n",
    "results = []\n",
    "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
    "for x in range(1, 11):\n",
    "    for y in range(1, 11):\n",
    "        count = df[(df[\"x\"] <= x) & (df[\"y\"] <= y)].shape[0]\n",
    "        results.append(count)\n",
    "pred = np.array(results).reshape(-1, 1) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61441fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = df.shape[0]\n",
    "n_column = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_5(grid, pred=None):\n",
    "    # 使用 numpy / calculate_query_cardinality_numpy / np.concatenate\n",
    "    if pred is None:\n",
    "        pred = m.predict(grid)\n",
    "    assert pred.shape[0] == grid.shape[0]\n",
    "    # generate by row, one query may generate several rows\n",
    "    column_names = [f\"col_{i}\" for i in range(n_column)]\n",
    "    # dataNew = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    count = 0\n",
    "    ArrayNew = None\n",
    "    ops = [\"<=\"] * n_column\n",
    "    pred = (pred * n_row).astype(int)  # Case 1: change 0.8 to 0, 1.8 to 1\n",
    "    for i in tqdm(range(grid.shape[0])):\n",
    "        vals = grid[i]\n",
    "        card = pred[i, 0] - calculate_query_cardinality_numpy(ArrayNew, ops, vals)\n",
    "\n",
    "        if card >= 1:\n",
    "            array3 = np.repeat(vals, card).reshape(n_column, card).T\n",
    "            ArrayNew = array3 if ArrayNew is None else np.concatenate((ArrayNew, array3), axis=0)\n",
    "            # dataNew = pd.DataFrame(ArrayNew, columns=column_names)\n",
    "            count += card\n",
    "            if count > n_row:\n",
    "                print(\n",
    "                    f\"Reached table max row length({n_row}) in {i}-th row of grid with grid value of {vals}, stop generation.\"\n",
    "                )\n",
    "                break\n",
    "    else:\n",
    "        print(\"Complete table generation\")\n",
    "        # if count < n_row:\n",
    "        #     print(\n",
    "        #         f\"Reached table max row length({n_row}) in the last row of grid, stop generation.\"\n",
    "        #     )\n",
    "        #     # 如果不足需要补 系统最大值\n",
    "        #     # dataNew = pd.DataFrame(ArrayNew, columns=column_names)\n",
    "        return pd.DataFrame(ArrayNew, columns=column_names)\n",
    "    return pd.DataFrame(ArrayNew, columns=column_names).iloc[:n_row, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew = generate_5(grid, pred)\n",
    "dataNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df79022",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNew.plot(kind=\"scatter\", x=\"col_0\", y=\"col_1\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9915b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main写成 SingleTable.py\n",
    "# 关于 class中的load方法，hdf5与h5的区别是什么？应该如何load？load权重还是模型结构一起load？哪一种更好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "806402ce",
   "metadata": {},
   "source": [
    "### generate_from_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确认一下，下面 batch生成的数组格式和grid 的格式是否是相同的，shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [v for v in unique_intervals.values()]\n",
    "mesh = np.meshgrid(*values)  # 所有unique interval 的笛卡尔积网格\n",
    "grid = np.array(mesh).T.reshape(-1, len(values)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe62ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 修改成 generate_from_batches(grid=None, pred=None)\n",
    "\n",
    "\n",
    "# 写一个def _generate_grid_batches 方法(下面代码块的重命名）\n",
    "def generate_batches(values, batch_size):\n",
    "    iterator = itertools.product(*values)\n",
    "    while True:\n",
    "        batch = list(itertools.islice(iterator, batch_size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield np.array(batch).astype(np.float32)\n",
    "\n",
    "\n",
    "# unique_intervals = {\n",
    "#     \"A\": list(range(1, 101)),\n",
    "#     \"B\": list(range(1, 101)),\n",
    "#     \"C\": list(range(1, 101)),\n",
    "#     \"D\": list(range(1, 101)),\n",
    "#     \"E\": list(range(1, 101)),\n",
    "# }\n",
    "values = [v for v in unique_intervals.values()]\n",
    "batch_size = 10000  # 根据实际需要调整批处理大小\n",
    "total_combinations = np.prod([len(v) for v in values])\n",
    "\n",
    "processed_batches = []\n",
    "for batch in tqdm(\n",
    "    generate_batches(values, batch_size), total=(total_combinations // batch_size) + 1\n",
    "):\n",
    "    processed_batch = batch  # 这里可以添加处理逻辑\n",
    "    processed_batches.append(processed_batch)\n",
    "\n",
    "# 将所有处理后的批次合并\n",
    "final_array = np.vstack(processed_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeafb7c",
   "metadata": {},
   "source": [
    "#### 为什么 kl散度是负数，修改 epoch为一个很小的值，看看是否会出现负数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可能与网络的初始化输出值有关？\n",
    "# 使用 kl散度是否时正确的，如何使用神经网络进行最大似然估计，似然与kl散度之间的关系是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6980010",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b989136",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37477b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7280f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024/06/06 update\n",
    "## 计算 Q-error 的速度：calculate_Q_error > calculate_Q_error_old, 二者的准确性相同, 目前的速度已经很快了\n",
    "## 生成表的速度：generate_3 > generate > generate_2，之后考虑用numpy的concatenate来替代pd的concat，进一步提升生成速度，第二个思路：如果采用 auto regressive 模型,按列生成是否可以借助 gpu 来加速，\n",
    "## 生成表的准确性：generate = generate_2 约等于 generate_3(有时候高，有时候低，误差不大，0.1与0.09999999的差别)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7c2dd",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把plot 整合到 lattice里，或者单独写几个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pred = m.predict(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(grid_pred, \"bo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15, 8))\n",
    "ax1 = plt.axes(projection=\"3d\")\n",
    "\n",
    "# xx = unique_intervals[1]\n",
    "# yy = unique_intervals[0]\n",
    "# X, Y = np.meshgrid(xx, yy)\n",
    "\n",
    "X = grid[:, 1].reshape(column_interval_number[0], column_interval_number[1])  # 这样也可以\n",
    "Y = grid[:, 0].reshape(column_interval_number[0], column_interval_number[1])\n",
    "Z = grid_pred.reshape(column_interval_number[0], column_interval_number[1])\n",
    "\n",
    "ax1.plot_surface(X, Y, Z, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1929bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "cs = ax2.contourf(X, Y, Z, cmap=\"viridis\")\n",
    "\n",
    "# Alternatively, you can manually set the levels\n",
    "# and the norm:\n",
    "# lev_exp = np.arange(np.floor(np.log10(z.min())-1),\n",
    "#                    np.ceil(np.log10(z.max())+1))\n",
    "# levs = np.power(10, lev_exp)\n",
    "# cs = ax.contourf(X, Y, z, levs, norm=colors.LogNorm())    # 这个是啥\n",
    "\n",
    "cbar = fig2.colorbar(cs)  # 让colorbar细粒度更高一点\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画一下原生的图做对比，是否需要更光滑\n",
    "# 变得光滑：\n",
    "# 1. 数据预处理，缩放，标准化\n",
    "# 2. 凸函数\n",
    "# 3. lattice正则器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e88c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query 对网格的覆盖率 散点图\n",
    "fig4 = plt.figure(figsize=(10, 10))\n",
    "xtick = unique_intervals[0]\n",
    "ytick = unique_intervals[1]\n",
    "plt.scatter(train_X[:, 0], train_X[:, 1], c=\"b\")\n",
    "plt.vlines(xtick, min(ytick), max(ytick), colors=\"green\")\n",
    "plt.hlines(ytick, min(xtick), max(xtick), colors=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59574b",
   "metadata": {},
   "source": [
    "# Lattice 其它尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68211d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比，对query做unique 和 不做unique的误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比传入table unique value 和 只传入 unique_intervals的模型优化效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table unique value\n",
    "data = table.data.to_numpy()\n",
    "unique_vals = []\n",
    "for i in range(data.shape[1]):\n",
    "    unique_vals.append(np.unique(data[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112dc94",
   "metadata": {},
   "source": [
    "# 尝试 tfl.configs.FeatureConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_mins = train_X.min(axis=0)\n",
    "# feat_maxs = train_X.max(axis=0)\n",
    "train = np.concatenate((train_X, train_Y), axis=1)\n",
    "train = np.unique(train, axis=0)\n",
    "train_X, train_Y = np.hsplit(train, [-1])\n",
    "df_train = pd.DataFrame(train, columns=[f\"col_{i}\" for i in range(train.shape[1] - 1)] + [\"sel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "    x=df_train,\n",
    "    y=df_train[\"sel\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# feature_analysis_input_fn is used for TF Lattice estimators.\n",
    "feature_analysis_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "    x=df_train,\n",
    "    y=df_train[\"sel\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column(\"col_0\"),\n",
    "    tf.feature_column.numeric_column(\"col_1\"),\n",
    "    tf.feature_column.numeric_column(\"col_2\"),\n",
    "]\n",
    "model_config = tfl.configs.CalibratedLatticeConfig(\n",
    "    feature_configs=[\n",
    "        tfl.configs.FeatureConfig(\n",
    "            name=\"col_0\",\n",
    "            lattice_size=2,\n",
    "            monotonicity=\"increasing\",\n",
    "            pwl_calibration_num_keypoints=1000,\n",
    "        ),\n",
    "        tfl.configs.FeatureConfig(\n",
    "            name=\"col_1\",\n",
    "            lattice_size=2,\n",
    "            monotonicity=\"increasing\",\n",
    "            pwl_calibration_num_keypoints=1000,\n",
    "        ),\n",
    "        tfl.configs.FeatureConfig(\n",
    "            name=\"col_1\",\n",
    "            lattice_size=2,\n",
    "            monotonicity=\"increasing\",\n",
    "            pwl_calibration_num_keypoints=1000,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "tfl_estimator = tfl.estimators.CannedClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    model_config=model_config,\n",
    "    feature_analysis_input_fn=feature_analysis_input_fn,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    config=tf.estimator.RunConfig(tf_random_seed=42),\n",
    ")\n",
    "tfl_estimator.train(input_fn=train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_visualize_lattice(tfl_estimator):\n",
    "    saved_model_path = tfl_estimator.export_saved_model(\n",
    "        \"/tmp/TensorFlow_Lattice_101/\",\n",
    "        tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "            feature_spec=tf.feature_column.make_parse_example_spec(feature_columns)\n",
    "        ),\n",
    "    )\n",
    "    model_graph = tfl.estimators.get_model_graph(saved_model_path)\n",
    "    figsize(8, 8)\n",
    "    tfl.visualization.draw_model_graph(model_graph)\n",
    "    return model_graph\n",
    "\n",
    "\n",
    "_ = save_and_visualize_lattice(tfl_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_lattice_input(table_size, query_set):\n",
    "    # Traverse all queries to apply the intervalization skill for each column\n",
    "    n_column = table_size[1]\n",
    "    x = [sys.maxsize] * n_column\n",
    "    for i in range(n_column):\n",
    "        column_interval[i] = set(\n",
    "            [0, sys.maxsize]\n",
    "        )  # use set([0, sys.maxsize]) to adapt '>' and '<'.\n",
    "    for query in query_set:\n",
    "        col_idxs = query[1]\n",
    "        vals = query[3]\n",
    "        for i in range(len(col_idxs)):\n",
    "            column_interval[col_idxs[i]].add(vals[i][0])\n",
    "    for k, v in column_interval.items():\n",
    "        if not v:\n",
    "            column_interval[k] = [0]\n",
    "        else:\n",
    "            column_interval[k] = sorted(list(v))\n",
    "    return column_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb031ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091da7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
