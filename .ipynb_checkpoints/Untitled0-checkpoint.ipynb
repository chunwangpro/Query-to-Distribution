{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osXtrhghacXi",
    "outputId": "e47da52e-e2fe-400e-e9fe-a1611a15718e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
      "Requirement already satisfied: tensorflow-lattice in /usr/local/lib/python3.7/dist-packages (2.0.10)\n",
      "Requirement already satisfied: pydoe in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.1+cu113)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.19.5)\n",
      "Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (2.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.0.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (0.15.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.3.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (3.2.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-lattice) (1.15.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pydoe) (1.7.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->tensorflow-lattice) (1.12.1)\n",
      "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->tensorflow-lattice) (0.8.10)\n",
      "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->tensorflow-lattice) (0.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tensorflow-lattice) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->tensorflow-lattice) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow-lattice) (2022.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->tensorflow-lattice) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->tensorflow-lattice) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz graphviz tensorflow-lattice pydoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M5vOQ7UmSJbm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import common\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "import estimators as estimators_lib\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "def Oracle(table, query):\n",
    "    cols, idxs, ops, vals = query\n",
    "    oracle_est = estimators_lib.Oracle(table)\n",
    "    return oracle_est.Query(cols, ops, vals)\n",
    "\n",
    "\n",
    "def cal_true_card(query, table):\n",
    "    cols, idxs, ops, vals = query\n",
    "    ops = np.array(ops)\n",
    "    probs = Oracle(table, (cols, idxs, ops, vals))\n",
    "    return probs\n",
    "\n",
    "\n",
    "def GenerateQuery(table, min_num_filters, max_num_filters, rng, dataset):\n",
    "    \"\"\"Generate a random query.\"\"\"\n",
    "    num_filters = rng.randint(max_num_filters - 1, max_num_filters)       ######### 这里可以改\n",
    "    cols, idxs, ops, vals = SampleTupleThenRandom(table, num_filters, rng,\n",
    "                                                  dataset)\n",
    "    sel = cal_true_card(\n",
    "        (cols, idxs, ops, vals), table) / float(table.cardinality)\n",
    "    return cols, idxs, ops, vals, sel\n",
    "\n",
    "\n",
    "def SampleTupleThenRandom(table, num_filters, rng, dataset):\n",
    "    vals = []\n",
    "    new_table = table.data\n",
    "    s = new_table.iloc[rng.randint(0, new_table.shape[0])]\n",
    "    vals = s.values\n",
    "\n",
    "    idxs = rng.choice(len(table.columns), replace=False, size=num_filters)   ######## 这里可以改\n",
    "    cols = np.take(table.columns, idxs)\n",
    "    # If dom size >= 10, okay to place a range filter.\n",
    "    # Otherwise, low domain size columns should be queried with equality.\n",
    "    #ops = rng.choice(['='], size=num_filters)\n",
    "    #ops = rng.choice(['<=', '>=', '>', '<'], size=num_filters)\n",
    "    #ops = rng.choice(['<=', '>='], size=num_filters)\n",
    "    ops = rng.choice(['<='], size=num_filters)\n",
    "    #ops_all_eqs = ['='] * num_filters\n",
    "    #sensible_to_do_range = [c.DistributionSize() >= 10 for c in cols]\n",
    "    #ops = np.where(sensible_to_do_range, ops, ops_all_eqs)\n",
    "    # if num_filters == len(table.columns):\n",
    "    #     return table.columns,np.arange(len(table.columns)), ops, vals\n",
    "    vals = vals[idxs]\n",
    "    op_a = []\n",
    "    val_a = []\n",
    "    for i in range(len(vals)):\n",
    "        val_a.append([vals[i]])\n",
    "        op_a.append([ops[i]])\n",
    "    return cols, idxs, pd.DataFrame(op_a).values, pd.DataFrame(val_a).values\n",
    "\n",
    "\n",
    "def dictionary_column_interval(table_size, query_set):\n",
    "    # Traverse all queries to apply the intervalization skill for each column\n",
    "    n_column = table_size[1]\n",
    "    column_interval = {}\n",
    "    for i in range(n_column):\n",
    "        column_interval[i] = set()\n",
    "    for query in query_set:\n",
    "        _, col_idxs, _, vals, _ = query\n",
    "        for i in range(len(col_idxs)):\n",
    "            column_interval[col_idxs[i]].add(vals[i][0])\n",
    "    for k, v in column_interval.items():\n",
    "        column_interval[k] = sorted(list(v))\n",
    "        least, great = column_interval[k][0], column_interval[k][-1]\n",
    "        column_interval[k] = sorted([0, least/2] + column_interval[k] + [great+1])\n",
    "    return column_interval\n",
    "\n",
    "\n",
    "def count_column_unique_interval(unique_intervals):\n",
    "    # count unique query interval in each column\n",
    "    return [len(v) for v in unique_intervals.values()]\n",
    "\n",
    "\n",
    "def process_train_data(unique_intervals, query_set, train_size=1):\n",
    "    train_size = 1\n",
    "    X, Y = [], []\n",
    "    origin = np.array([[0, v[-1]] for v in unique_intervals.values()]).ravel()\n",
    "    for query in query_set:\n",
    "        x = list(origin)\n",
    "        _, col_idxs, ops, vals, sel = query\n",
    "        for i in range(len(col_idxs)):\n",
    "            if ops[i][0] == \"<=\":\n",
    "                x[col_idxs[i]*2+1] = vals[i][0]\n",
    "            elif ops[i][0] == \"<\":\n",
    "                ind = unique_intervals[col_idxs[i]].index(vals[i][0]) - 1\n",
    "                x[col_idxs[i]*2+1] = unique_intervals[col_idxs[i]][ind]\n",
    "            elif ops[i][0] == \">\":\n",
    "                x[col_idxs[i]*2] = vals[i][0]\n",
    "            elif ops[i][0] == \">=\":\n",
    "                ind = unique_intervals[col_idxs[i]].index(vals[i][0]) + 1\n",
    "                x[col_idxs[i]*2] = unique_intervals[col_idxs[i]][ind]\n",
    "            elif ops[i][0] == \"=\":\n",
    "                ind = unique_intervals[col_idxs[i]].index(vals[i][0]) - 1\n",
    "                x[col_idxs[i]*2] = unique_intervals[col_idxs[i]][ind]\n",
    "                x[col_idxs[i]*2+1] = vals[i][0]\n",
    "        X.append(x)\n",
    "        Y.append(sel)\n",
    "    X = np.array(X).astype(np.float32)\n",
    "    Y = np.array(Y).astype(np.float32).reshape(-1, 1)\n",
    "    total = np.concatenate((X, Y), axis=1)\n",
    "    # total = np.unique(total, axis=0)\n",
    "#     choose = np.random.choice(total.shape[0], size=round(total.shape[0]*train_size), replace=False)\n",
    "#     others = list(set(range(total.shape[0])) - set(choose))\n",
    "#     train, test = total[choose], total[others]\n",
    "#     df_train = pd.DataFrame(train, columns=[f'col_{i}' for i in range(total.shape[1])])\n",
    "    df_train = pd.DataFrame(total, columns=[f'col_{i}' for i in range(total.shape[1])])\n",
    "    # boundary\n",
    "    df_train.loc[len(df_train.index)] = [0] * total.shape[1]\n",
    "    zero = [[v[-1], 0] for v in unique_intervals.values()]\n",
    "    df_train.loc[len(df_train.index)] = list(np.array(zero).ravel()) + [0.0]\n",
    "    one = [[0, v[-1]] for v in unique_intervals.values()]\n",
    "    df_train.loc[len(df_train.index)] = list(np.array(one).ravel()) + [1.0]\n",
    "    \n",
    "    new_train = np.array(df_train.sort_values(by=list(df_train.columns)[:-1]))\n",
    "    train_X, train_Y = np.hsplit(new_train, [-1])\n",
    "\n",
    "#     df_test = pd.DataFrame(test, columns=[f'col_{i}' for i in range(total.shape[1])])\n",
    "#     new_test = np.array(df_test.sort_values(by=list(df_test.columns)[:-1]))\n",
    "#     test_X, test_Y = np.hsplit(new_test, [-1])\n",
    "    return train_X, train_Y  # , test_X, test_Y\n",
    "\n",
    "\n",
    "def generate_data_new(grid, model):\n",
    "    assert grid.shape[1] == n_column\n",
    "    # transform 1-input grid to 2-input extend grid\n",
    "    length = grid.shape[0]\n",
    "    inf = [0] * length\n",
    "    grid_dict = {}\n",
    "    for i in range(n_column):\n",
    "        grid_dict[f'col_{i}_inf'] = inf\n",
    "        grid_dict[f'col_{i}_sup'] = grid[:, i]\n",
    "    extend_grid = np.array(pd.DataFrame(grid_dict))\n",
    "    print(\"Begin model inference\")\n",
    "    pred = model.inference(extend_grid)\n",
    "    print(\"Done\")\n",
    "    # newpred is the predict cardinality\n",
    "    newpred = np.round(pred*n_row)\n",
    "    #newpred = np.round(pred)\n",
    "    # delete all the zero cardinality rows\n",
    "    line = pd.DataFrame(\n",
    "        np.concatenate([grid, newpred], axis=1),\n",
    "        columns=[f'col_{i}' for i in range(n_column)]+['card']\n",
    "    )\n",
    "    nozero = (line == 0).sum(axis=1)\n",
    "    line = line[nozero==0].reset_index(drop=True)\n",
    "    grid, pred = np.hsplit(np.array(line), [-1])\n",
    "    pred = pred.astype(np.int)\n",
    "    # generate dataNew\n",
    "    print(\"\\nBegin generating table...\")\n",
    "    dataNew = pd.DataFrame(\n",
    "        columns=[f'col_{i}' for i in range(n_column)],\n",
    "        index=range(n_row)\n",
    "    )\n",
    "    count = 0\n",
    "    for i in trange(grid.shape[0]):\n",
    "        df = dataNew\n",
    "        grid_value = grid[i]\n",
    "        for j in range(n_column):\n",
    "            df = df.query(f'col_{j} <= {grid_value[j]}')\n",
    "        card = pred[i][0] - df.shape[0]\n",
    "        if card > 0:\n",
    "            #df3 = pd.DataFrame({f\"col_{k}\": [grid_value[k]] * card for k in range(n_column)})\n",
    "            #dataNew = dataNew.append(df3, ignore_index = True)\n",
    "            dataNew.iloc[count:count + card, :] = grid_value\n",
    "            count += card\n",
    "            if count > n_row:\n",
    "                print(\"Reached table length in \", i, grid.shape[0])\n",
    "                break\n",
    "        # print table length every 5000\n",
    "        if i % 5000 == 0:\n",
    "            print(count)\n",
    "    dataNew.dropna(axis=0, how='all', inplace=True)\n",
    "    return dataNew\n",
    "    \n",
    "    \n",
    "def execute_query(dataNew, query_set):\n",
    "    diff = []\n",
    "    for query in tqdm(query_set):\n",
    "        df = dataNew\n",
    "        _, col_idxs, ops, vals, sel = query\n",
    "        for i in range(len(col_idxs)):\n",
    "            op = '==' if ops[i][0] == \"=\" else ops[i][0]\n",
    "            df = df.query(f'col_{col_idxs[i]} {op} {vals[i][0]}')\n",
    "        card = 1 if round(sel * n_row) == 0 else round(sel * n_row)\n",
    "        card2 = 1 if df.shape[0] == 0 else df.shape[0]\n",
    "        diff.append(max(card/card2, card2/card))\n",
    "    return diff\n",
    "\n",
    "\n",
    "def print_error(diff, args):\n",
    "    print(\n",
    "        f\"\\n\\n Q-error of Lattice (query size={args.query_size}, condition={args.num_conditions}, epoches={args.epochs}):\\n\"\n",
    "    )\n",
    "    print(f\"min:    {np.min(diff)}\")\n",
    "    print(f\"10:     {np.percentile(diff, 10)}\")\n",
    "    print(f\"20:     {np.percentile(diff, 20)}\")\n",
    "    print(f\"30:     {np.percentile(diff, 30)}\")\n",
    "    print(f\"40:     {np.percentile(diff, 40)}\")\n",
    "    print(f\"median: {np.median(diff)}\")\n",
    "    print(f\"60:     {np.percentile(diff, 60)}\")\n",
    "    print(f\"70:     {np.percentile(diff, 70)}\")\n",
    "    print(f\"80:     {np.percentile(diff, 80)}\")\n",
    "    print(f\"90:     {np.percentile(diff, 90)}\")\n",
    "    print(f\"95:     {np.percentile(diff, 95)}\")\n",
    "    print(f\"max:    {np.max(diff)}\")\n",
    "    print(f\"mean:   {np.mean(diff)}\")\n",
    "    \n",
    "    \n",
    "def calc_time(tic, toc):\n",
    "    total_time = toc - tic\n",
    "    m, s = divmod(total_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:0>2.0f}:{m:0>2.0f}:{s:0>2.0f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sZvNz2H2Tr5I"
   },
   "outputs": [],
   "source": [
    "class LatticeCDFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, lattice_size=2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.lattice_size = lattice_size\n",
    "        \n",
    "        self.copula_lattice = tfl.layers.Lattice(\n",
    "            lattice_sizes=[self.lattice_size] * self.dim,\n",
    "            interpolation='hypercube',  # simplex\n",
    "            monotonicities=['increasing'] * self.dim,\n",
    "            output_min= 0.0,\n",
    "            output_max= 1.0,\n",
    "            name='lattice',\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self.copula_lattice(x)\n",
    "        grad = y\n",
    "        for i in range(self.dim):\n",
    "            grad = tf.gradients(grad, x[i])#, stop_gradients=[a, b])\n",
    "            grad = grad[0]\n",
    "        return y, x, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V-zEzNria55G"
   },
   "outputs": [],
   "source": [
    "class CopulaModel(tf.keras.Model):\n",
    "    def __init__(self, dim, lattice_size=2, pwl_keypoints=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.copula_lattice = LatticeCDFLayer(dim=dim, lattice_size=lattice_size)\n",
    "        self.pwl_calibration_input_keypoints = unique_intervals if pwl_keypoints is None else pwl_keypoints\n",
    "        \n",
    "        self.model_inputs = []\n",
    "        for i in range(self.dim):\n",
    "            self.model_inputs.append(\n",
    "                tf.keras.layers.Input(shape=[1], name='col_%s_inf' % i))\n",
    "            self.model_inputs.append(\n",
    "                tf.keras.layers.Input(shape=[1], name='col_%s_sup' % i))\n",
    "\n",
    "        self.calibrators = []\n",
    "        for i in range(self.dim):\n",
    "            self.calibrators.append(\n",
    "                tfl.layers.PWLCalibration(\n",
    "                    input_keypoints=np.array(\n",
    "                        self.pwl_calibration_input_keypoints[i]),\n",
    "                    dtype=tf.float32,\n",
    "                    output_min=0.0,\n",
    "                    output_max=1.0,\n",
    "                    clamp_min=True,\n",
    "                    clamp_max=True,\n",
    "                    monotonicity='decreasing',\n",
    "                    name='col_%s_inf_pwl' % i\n",
    "                )(self.model_inputs[2*i]))\n",
    "            self.calibrators.append(\n",
    "                tfl.layers.PWLCalibration(\n",
    "                    input_keypoints=np.array(\n",
    "                        self.pwl_calibration_input_keypoints[i]),\n",
    "                    # input_keypoints=np.linspace(\n",
    "                    #     feat_mins[i],\n",
    "                    #     feat_maxs[i],\n",
    "                    #     num=pwl_calibration_num_keypoints),\n",
    "                    dtype=tf.float32,\n",
    "                    output_min=0.0,\n",
    "                    output_max=1.0,\n",
    "                    clamp_min=True,\n",
    "                    clamp_max=True,\n",
    "                    monotonicity='increasing',\n",
    "                    name='col_%s_sup_pwl' % i\n",
    "                )(self.model_inputs[2*i+1]))\n",
    "\n",
    "        self.lattice_cdf = []\n",
    "        for i in range(self.dim):\n",
    "            self.lattice_cdf.append(\n",
    "                tfl.layers.Lattice(\n",
    "                    lattice_sizes=[lattice_size] * 2,\n",
    "                    interpolation='hypercube',  # simplex\n",
    "                    monotonicities=['increasing'] * 2,\n",
    "                    output_min= 0.0,\n",
    "                    output_max= 1.0,\n",
    "                    name='lattice_col_%s' % i,\n",
    "                )([self.calibrators[2*i], self.calibrators[2*i+1]]))\n",
    "            \n",
    "        self.model = tf.keras.models.Model(\n",
    "            inputs=self.model_inputs,\n",
    "            outputs= self.copula_lattice(self.lattice_cdf)\n",
    "        )\n",
    "        #self.model.save('%s.hdf5' % self.model_path)\n",
    "        self.model.summary()\n",
    "    \n",
    "        \n",
    "    def compile(self, loss, optimizer):\n",
    "        super().compile()\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, lattice_inputs, lattice_grad = self.model(x)\n",
    "            print(y_pred)\n",
    "            print(y_pred.shape)\n",
    "            loss1 = self.loss(y, y_pred)\n",
    "            loss2 = tf.nn.relu(y_pred - tf.keras.backend.min(lattice_inputs))\n",
    "            lb = tf.math.maximum(sum(x) - self.dim + 1, 0)\n",
    "            loss3 = tf.nn.relu(lb - y_pred)\n",
    "            loss4 = tf.nn.relu(-lattice_grad[0])\n",
    "            loss = loss1 + loss2 + loss3 + loss4\n",
    "        trainable_vars = self.model.trainable_weights\n",
    "        grads = tape.gradient(loss1, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        return {\"loss\": loss, \"loss1\": loss1, \"loss2\": loss2, \"loss3\": loss3, \"loss4\": loss4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yd2DHdvka-AF"
   },
   "outputs": [],
   "source": [
    "class Trainer_Lattice:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        dim,\n",
    "        pwl_keypoints=None,  # also can input table unique values\n",
    "        lattice_size=2):\n",
    "        self.model = CopulaModel(dim=dim, lattice_size=2, pwl_keypoints=None)\n",
    "        self.model_path = './models/Lattice/model/' + name\n",
    "        self.weight_path = './models/Lattice/weight/' + name\n",
    "        self.pwl_calibration_input_keypoints = unique_intervals if pwl_keypoints is None else pwl_keypoints\n",
    "        \n",
    "        \n",
    "    def train(self,\n",
    "              X,\n",
    "              y,\n",
    "              lr=0.01,\n",
    "              bs=20,\n",
    "              epochs=3000,\n",
    "              reduceLR_factor=0.5,\n",
    "              reduceLR_patience=20,\n",
    "              earlyStopping_patience=200,\n",
    "              verbose=1,\n",
    "              loss='MSE',\n",
    "              opt='Adam'):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "        features = [X[:, i] for i in range(X.shape[1])]\n",
    "        target = y\n",
    "        Loss = {\n",
    "            'MAE': tf.keras.losses.mean_absolute_error,\n",
    "            'MSE': tf.keras.losses.mean_squared_error,\n",
    "            'MAPE': tf.keras.losses.mean_absolute_percentage_error\n",
    "        }\n",
    "\n",
    "        Opt = {\n",
    "            'Adam': tf.keras.optimizers.Adam(),\n",
    "            'Nadam': tf.keras.optimizers.Nadam(),\n",
    "            'Adagrad': tf.keras.optimizers.Adagrad(),\n",
    "            'Adadelta': tf.keras.optimizers.Adadelta(),\n",
    "            'Adamax': tf.keras.optimizers.Adamax(),\n",
    "            'RMSprop': tf.keras.optimizers.RMSprop(),\n",
    "        }\n",
    "        self.model.compile(loss=Loss[loss], optimizer=Opt[opt])\n",
    "\n",
    "        #self.model.save('%s' % self.model_path, save_format='tf')\n",
    "        \n",
    "        \n",
    "\n",
    "        earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                         patience=earlyStopping_patience,\n",
    "                                                         verbose=verbose,\n",
    "                                                         mode='min')\n",
    "        mcp_save = tf.keras.callbacks.ModelCheckpoint('%s.hdf5' %\n",
    "                                                      self.weight_path,\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor='loss',\n",
    "                                                      mode='min',\n",
    "                                                      save_weights_only=True)\n",
    "        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=reduceLR_factor,\n",
    "            patience=reduceLR_patience,\n",
    "            verbose=verbose,\n",
    "            epsilon=1e-15,\n",
    "            mode='min')\n",
    "\n",
    "        self.model.fit(features,\n",
    "                       target,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=bs,\n",
    "                       verbose=1,\n",
    "                       callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n",
    "                       )\n",
    "        self.model.load_weights('%s.hdf5' % self.weight_path)\n",
    "\n",
    "        \n",
    "    def load(self):\n",
    "        self.model = tf.keras.models.load_model('%s.hdf5' % self.model_path)\n",
    "        self.model.load_weights('%s.hdf5' % self.weight_path)\n",
    "\n",
    "        \n",
    "    def inference(self, grid):\n",
    "        assert grid.shape[1] == self.dim * 2\n",
    "        pred = self.model.predict(np.hsplit(grid, self.dim * 2))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "56e0fb56e55a45a690846e3716498308",
      "71e3206527674ad696666fd3f0d827f8",
      "69d77944b1604935b9ca0916448d7235",
      "627ad57640a24ea79e3e47726d8d492e",
      "92136ba2a1244b389693fd16833a4355",
      "189f8ee171524762baf220e408d64978",
      "817d742e18ee4cdaa15cacf07326555f",
      "7385de9895d54e599db3082fc623305c",
      "5fa535a1615440baae644ea964fb7a2f",
      "a54ada9022084c6a9ac16cfc8a5c82c4",
      "3c3b0d58a6b745509eeed74e37feba9f"
     ]
    },
    "id": "mW2SpqH7bFen",
    "outputId": "a9e48314-e131-4654-f2b9-969f17d51271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset wine2.csv done\n",
      "(6497, 2)\n",
      "(6497, 2)\n",
      "0\n",
      "1\n",
      "0 106\n",
      "1 187\n",
      "Begin Generating Queries ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e531cd1fc1bd494fae6973d990bf4dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete Generating Queries.\n",
      "\n",
      "\n",
      "Calculating intervalization...\n",
      "\n",
      "Column intervals [85, 112] 9520\n",
      "\n",
      "\n",
      "Building Lattice...\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "col_0_inf (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col_0_sup (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col_1_inf (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col_1_sup (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "col_0_inf_pwl (PWLCalibration)  (None, 1)            85          col_0_inf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "col_0_sup_pwl (PWLCalibration)  (None, 1)            85          col_0_sup[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "col_1_inf_pwl (PWLCalibration)  (None, 1)            112         col_1_inf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "col_1_sup_pwl (PWLCalibration)  (None, 1)            112         col_1_sup[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lattice_col_0 (Lattice)         (None, 1)            4           col_0_inf_pwl[0][0]              \n",
      "                                                                 col_0_sup_pwl[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lattice_col_1 (Lattice)         (None, 1)            4           col_1_inf_pwl[0][0]              \n",
      "                                                                 col_1_sup_pwl[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lattice_cdf_layer_3 (LatticeCDF ((None, 1), ListWrap 4           lattice_col_0[0][0]              \n",
      "                                                                 lattice_col_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 406\n",
      "Trainable params: 406\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "Lattice is already built, begin training...\n",
      "\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compile() got an unexpected keyword argument 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-860a4319ea89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             )\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a14fc38a28f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, lr, bs, epochs, reduceLR_factor, reduceLR_patience, earlyStopping_patience, verbose, loss, opt)\u001b[0m\n\u001b[1;32m     74\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                        )\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.hdf5'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2358\u001b[0m     \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2361\u001b[0m       \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_from_inputs\u001b[0;34m(self, all_inputs, target, orig_inputs, orig_target)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m         experimental_run_tf_function=self._experimental_run_tf_function)\n\u001b[0m\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m   \u001b[0;31m# TODO(omalleyt): Consider changing to a more descriptive function name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compile() got an unexpected keyword argument 'metrics'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset',\n",
    "                        type=str,\n",
    "                        default='wine2',\n",
    "                        help='Dataset.')\n",
    "    parser.add_argument('--loss', type=str, default='MSE', help='Loss.')\n",
    "    parser.add_argument('--opt', type=str, default='Adam', help='Optimizer.')\n",
    "    parser.add_argument('--query-size',\n",
    "                        type=int,\n",
    "                        default=2000,\n",
    "                        help='query size')\n",
    "    parser.add_argument('--num-conditions',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='num of conditions')\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=2000,\n",
    "                        help='Number of epochs to train for.')\n",
    "    parser.add_argument('--lhs_n',\n",
    "                        type=int,\n",
    "                        default=10000,\n",
    "                        help='Number of lhs samples to generate for.')\n",
    "    parser.add_argument('--train-size',\n",
    "                        type=float,\n",
    "                        default=0.8,\n",
    "                        help='train size')\n",
    "    parser.add_argument('--bs', type=int, default=1, help='Batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate')\n",
    "    parser.add_argument('--lattice', type=int, default=2, help='Lattice size.')\n",
    "    parser.add_argument('--seed', type=int, default=4321, help='Random seed')\n",
    "    parser.add_argument('--sample',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help='reload trained mode')\n",
    "    #args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    bs = int(args.bs)\n",
    "    lr = float(args.lr)\n",
    "    train_size = float(args.train_size)\n",
    "    epochs = int(args.epochs)\n",
    "    lattice = int(args.lattice)\n",
    "    sample = int(args.sample)\n",
    "    lhs_n = int(args.lhs_n)\n",
    "\n",
    "    type_casts = {}\n",
    "    table = datasets.LoadDataset(args.dataset + '.csv',\n",
    "                                 args.dataset,\n",
    "                                 type_casts=type_casts)\n",
    "    \n",
    "    print('Begin Generating Queries ...')\n",
    "    time0 = time.time()\n",
    "    rng = np.random.RandomState(args.seed)\n",
    "    query_set = [\n",
    "        GenerateQuery(table, 2, args.num_conditions + 1, rng, args.dataset)\n",
    "        for i in trange(args.query_size)\n",
    "    ]\n",
    "    print('Complete Generating Queries.')\n",
    "\n",
    "    print(\"\\n\\nCalculating intervalization...\")\n",
    "    time1 = time.time()\n",
    "    table_size = table.data.shape\n",
    "    n_row, n_column = table_size[0], table_size[1]\n",
    "    unique_intervals = dictionary_column_interval(table_size, query_set)\n",
    "    column_interval_number = count_column_unique_interval(unique_intervals)\n",
    "    print(\"\\nColumn intervals\", column_interval_number, np.product(column_interval_number))\n",
    "    \n",
    "    print(\"\\n\\nBuilding Lattice...\")\n",
    "    train_X, train_Y = process_train_data(unique_intervals, query_set)\n",
    "    # train_X, train_Y, test_X, test_Y = process_train_data(unique_intervals, query_set)\n",
    "    # print(\"  Total query:\", args.query_size)\n",
    "    # print(\"  Train query:\", train_X.shape[0])\n",
    "    # print(\"  Test  query:\", test_X.shape[0])\n",
    "    # print(\"\\n\\n\")\n",
    "    name = f\"{args.dataset}_{args.query_size}query_{args.num_conditions}column_{args.epochs}epoch\"\n",
    "    #model = LatticeCDF(unique_intervals, pwl_keypoints=None)\n",
    "    m = Trainer_Lattice(name, n_column, pwl_keypoints=None)\n",
    "    \n",
    "    print(\"\\n\\nLattice is already built, begin training...\\n\")\n",
    "    time2 = time.time()\n",
    "    m.train(train_X,\n",
    "            train_Y,\n",
    "            lr=lr,\n",
    "            bs=bs,\n",
    "            epochs=epochs,\n",
    "            loss=args.loss,\n",
    "            opt=args.opt\n",
    "            )\n",
    "\n",
    "    print(\"\\nFinish training\")\n",
    "    time3 = time.time()\n",
    "    # Full-Factorial net of unique intervals\n",
    "#     values = [v for v in unique_intervals.values()]\n",
    "#     mesh = np.meshgrid(*values)\n",
    "#     grid = np.array(mesh).T.reshape(-1, len(values)).astype(np.float32)\n",
    "    \n",
    "    # Latin Hypercube sampling\n",
    "#     lb = np.array([v[1] for v in unique_intervals.values()])\n",
    "#     ub = np.array([v[-1] for v in unique_intervals.values()])\n",
    "#     lhs_sample = lhs(n_column, samples=10000, criterion='center')\n",
    "#     sample_df = pd.DataFrame(lb + (ub-lb)*lhs_sample, columns=[f'col_{i}' for i in range(n_column)])\n",
    "#     grid = np.array(sample_df.sort_values(by=list(sample_df.columns)))\n",
    "    \n",
    "    \n",
    "    lb = np.array([1] * n_column)\n",
    "    ub = np.array(column_interval_number) - 1\n",
    "    lhs_sample = lb + (ub - lb) * lhs(n_column, samples=lhs_n, criterion='center')\n",
    "    index = np.round(lhs_sample).astype(np.int)\n",
    "    grid_mesh = np.empty_like(index, dtype=np.float)\n",
    "    for i in range(lhs_n):\n",
    "        idx = index[i, :]\n",
    "        grid_mesh[i] = [unique_intervals[j][idx[j]] for j in range(n_column)]\n",
    "    sample_df = pd.DataFrame(grid_mesh, columns=[f'col_{i}' for i in range(n_column)])\n",
    "    grid_a = np.array(sample_df.sort_values(by=list(sample_df.columns)))\n",
    "    greatest = np.array([v[-1] for v in unique_intervals.values()]).reshape(1, -1)\n",
    "    grid = np.concatenate([grid_a, greatest], axis=0)\n",
    "    dataNew = generate_data_new(grid, model=m)\n",
    "\n",
    "    print(\"\\nFinish generate table, calculating Q-error on new table...\")\n",
    "    time4 = time.time()\n",
    "    diff = execute_query(dataNew, query_set)\n",
    "    print_error(diff, args)\n",
    "\n",
    "    print(f\"\\noriginal table shape: {table_size}\")\n",
    "    print(f\"  Our table shape   : {dataNew.shape}\")\n",
    "    time5 = time.time()\n",
    "\n",
    "    print(\"\\nTime passed:\")\n",
    "    print(\" Generate Query  :  \", calc_time(time0, time1))\n",
    "    print(\" Build  Lattice  :  \", calc_time(time1, time2))\n",
    "    print(\"   Training      :  \", calc_time(time2, time3))\n",
    "    print(\"Generate New Data:  \", calc_time(time3, time4))\n",
    "    print(\"Calculate Q-error:  \", calc_time(time4, time5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtxjzk6aPNcX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "189f8ee171524762baf220e408d64978": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3b0d58a6b745509eeed74e37feba9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56e0fb56e55a45a690846e3716498308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71e3206527674ad696666fd3f0d827f8",
       "IPY_MODEL_69d77944b1604935b9ca0916448d7235",
       "IPY_MODEL_627ad57640a24ea79e3e47726d8d492e"
      ],
      "layout": "IPY_MODEL_92136ba2a1244b389693fd16833a4355"
     }
    },
    "5fa535a1615440baae644ea964fb7a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "627ad57640a24ea79e3e47726d8d492e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a54ada9022084c6a9ac16cfc8a5c82c4",
      "placeholder": "​",
      "style": "IPY_MODEL_3c3b0d58a6b745509eeed74e37feba9f",
      "value": " 2000/2000 [00:03&lt;00:00, 560.60it/s]"
     }
    },
    "69d77944b1604935b9ca0916448d7235": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7385de9895d54e599db3082fc623305c",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5fa535a1615440baae644ea964fb7a2f",
      "value": 2000
     }
    },
    "71e3206527674ad696666fd3f0d827f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_189f8ee171524762baf220e408d64978",
      "placeholder": "​",
      "style": "IPY_MODEL_817d742e18ee4cdaa15cacf07326555f",
      "value": "100%"
     }
    },
    "7385de9895d54e599db3082fc623305c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "817d742e18ee4cdaa15cacf07326555f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92136ba2a1244b389693fd16833a4355": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a54ada9022084c6a9ac16cfc8a5c82c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
